{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U accelerate bitsandbytes git+https://github.com/huggingface/transformers.git\n",
    "!pip install datasets -q\n",
    "!pip install peft -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SkinCAP'...\n",
      "Filtering content: 100% (4347/4347), 545.21 MiB | 11.78 MiB/s, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://brucewayne0459:your_hf_token@huggingface.co/datasets/joshuachou/SkinCAP  #Replace with your hf token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ebaac759a24768a224f9c2a5af3f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (2.3.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp) (3.7)\n",
      "Requirement already satisfied: nest_asyncio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install aiohttp\n",
    "!pip install nest_asyncio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from PIL import Image\n",
    "import os\n",
    "from io import BytesIO\n",
    "from functools import lru_cache\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "jsonl_file_path = \"/teamspace/studios/this_studio/Updated_Final_Cleaned_Skin_Diseases_Data_with_question_v2.jsonl\"\n",
    "dataset = load_dataset('json', data_files=jsonl_file_path, split='train')\n",
    "\n",
    "# Step 2: Split the dataset into training and validation sets\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds = dataset[\"test\"]\n",
    "\n",
    "# Step 3: Define the base directory where images are stored\n",
    "image_base_dir = \"/teamspace/studios/this_studio/SkinCAP/skincap\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Asynchronous function to load images from URLs\n",
    "async def async_load_image_from_url(session, url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        async with session.get(url, headers=headers) as response:\n",
    "            if response.status == 200:\n",
    "                img_data = await response.read()\n",
    "                img = Image.open(BytesIO(img_data)).convert(\"RGB\")\n",
    "                return img\n",
    "            else:\n",
    "                print(f\"URL not accessible (Status Code: {response.status}): {url}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "@lru_cache(maxsize=1024)  # Cache up to 1024 images in memory\n",
    "def load_image_from_cache(img_filename):\n",
    "    if img_filename.startswith(\"http\"):\n",
    "        return None  # URLs are handled asynchronously, no need to cache them here\n",
    "    else:\n",
    "        img_path = os.path.join(image_base_dir, img_filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                return img\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_filename} from path {img_path}: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"File not found: {img_filename} (Path: {img_path})\")\n",
    "            return None\n",
    "\n",
    "async def fetch_images(examples):\n",
    "    images = []\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for example in examples:\n",
    "            img_filename = example[\"image\"]\n",
    "            if img_filename.startswith(\"http\"):\n",
    "                tasks.append(async_load_image_from_url(session, img_filename))\n",
    "            else:\n",
    "                img = load_image_from_cache(img_filename)\n",
    "                tasks.append(asyncio.sleep(0))  # Dummy task to keep indices aligned\n",
    "                images.append(img)\n",
    "                texts.append(example[\"prefix\"])\n",
    "                labels.append(example[\"suffix\"])\n",
    "\n",
    "        fetched_images = await asyncio.gather(*tasks)\n",
    "\n",
    "        valid_images = []\n",
    "        valid_texts = []\n",
    "        valid_labels = []\n",
    "\n",
    "        for i, img in enumerate(fetched_images):\n",
    "            if img is not None:\n",
    "                valid_images.append(img)\n",
    "                valid_texts.append(examples[i][\"prefix\"])\n",
    "                valid_labels.append(examples[i][\"suffix\"])\n",
    "\n",
    "    return valid_images, valid_texts, valid_labels\n",
    "\n",
    "def collate_fn(examples):\n",
    "    try:\n",
    "        # This allows nesting of asyncio in environments that already have an event loop running\n",
    "        nest_asyncio.apply()\n",
    "        images, texts, labels = asyncio.run(fetch_images(examples))\n",
    "    except RuntimeError:  # If an event loop is already running, use another approach\n",
    "        loop = asyncio.get_event_loop()\n",
    "        images, texts, labels = loop.run_until_complete(fetch_images(examples))\n",
    "\n",
    "    # Use the processor to handle tokenization and image processing\n",
    "    tokens = processor(text=texts, images=images, suffix=labels,\n",
    "                       return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "    # Move tokens to GPU and convert to bfloat16\n",
    "    tokens = {key: value.to(device) for key, value in tokens.items()}\n",
    "\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load the processor\n",
    "model_id = \"google/paligemma-3b-pt-224\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Step 6: Setup Bits and Bytes Config for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Step 7: Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Step 8: Load the model with quantization and apply LoRA\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\": 0})\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Show trainable parameters for LoRA\n",
    "\n",
    "device = \"cuda\"\n",
    "image_token = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9: Define the training arguments\n",
    "output_dir = \"./pali_gemma_derm\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=6,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=1e-6,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",  # Save the model after every epoch\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model after every epoch\n",
    "    save_total_limit=3,  # Keeps the last 3 checkpoints\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    "    bf16=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    dataloader_pin_memory=False,  # Disable pinning memory\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Check if there's a checkpoint to resume from\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(output_dir) and len(os.listdir(output_dir)) > 0:\n",
    "    last_checkpoint = max([os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint\")], key=os.path.getctime)\n",
    "    print(f\"Resuming from checkpoint: {last_checkpoint}\")\n",
    "\n",
    "# Step 10: Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    args=training_args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd99b49458a642f2b773de48ef661ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fefec2704b40d498b32910b5bc5266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668416dd38b0435291996f68232841b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f8d9bf058d472d88db0d5f5df77b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628b4903ba3243c699755641ec642a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ceb264a371c49b4aec4660f65a650c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d587354bca646968514fe1727a4eed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9e0e5bbbd74f42a71e45a0c6ad7767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf12d25ea7745edac05aab4f127d046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/62.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee04596ffcd4aba9644c4edf46d789a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b554e7cef1944eab0badd664453796b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd5798bdf9d46d18b79abdfc83a0da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f008ee2b4424bb09a93b8f4cc6846eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e48ed1b0e2473e8e45499468ea6271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddcdd4a4e174ff1a40b2d2d4bca5487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,298,816 || all params: 2,934,765,296 || trainable%: 0.3850\n",
      "Resuming from checkpoint: ./pali_gemma_derm/checkpoint-5383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1527: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7690' max='7690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7690/7690 4:29:57, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.261300</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.243250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found:  (Path: /teamspace/studios/this_studio/SkinCAP/skincap/)\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=6724\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=8364\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2863\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2856\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=4031\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=4030\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2862\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2859\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/paligemma/configuration_paligemma.py:146: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.44, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=6724\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2863\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=4030\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=4031\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=8364\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2862\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2856\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2859\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/paligemma/configuration_paligemma.py:146: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.44, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2862\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=8364\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=4030\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=4031\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=6724\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2863\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/paligemma/configuration_paligemma.py:146: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.44, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2859\n",
      "URL not accessible (Status Code: 404): http://atlasdermatologico.com.br/img?imageId=2860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/paligemma/configuration_paligemma.py:146: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.44, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7690, training_loss=0.07154164264664073, metrics={'train_runtime': 16205.8291, 'train_samples_per_second': 11.396, 'train_steps_per_second': 0.475, 'total_flos': 5.774222626298077e+17, 'train_loss': 0.07154164264664073, 'epoch': 9.998050682261209})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 11: Start training or resume from checkpoint\n",
    "trainer.train(resume_from_checkpoint=last_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9061a0f210c1449e9fa76d59d86e0276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PaliGemmaForConditionalGeneration(\n",
       "  (vision_tower): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(256, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (self_attn): SiglipSdpaAttention(\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): PaliGemmaMultiModalProjector(\n",
       "    (linear): Linear(in_features=1152, out_features=2048, bias=True)\n",
       "  )\n",
       "  (language_model): GemmaForCausalLM(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(257216, 2048, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaSdpaAttention(\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (k_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (o_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (rotary_emb): GemmaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=16384, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=16384, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "          (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=257216, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Load the processor and model\n",
    "model_id = \"google/paligemma-3b-pt-224\"\n",
    "processor = AutoProcessor.from_pretrained(model_id,device_map={\"\": 0})\n",
    "\n",
    "# Load the model with LoRA adapters\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\"/teamspace/studios/this_studio/pali_gemma_derm/checkpoint-7690\",device_map = 'cuda')\n",
    "model.eval()  # Set model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: Identify the skin disease?\n",
      "vitiligo\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Identify the skin disease?\"\n",
    "input_image_path = \"/teamspace/studios/this_studio/vitiligo-0011.jpg\"  # Replace with your image path\n",
    "input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Process the input\n",
    "inputs = processor(text=input_text, images=input_image, return_tensors=\"pt\", padding=\"longest\").to(\"cuda\")\n",
    "\n",
    "# Run inference with increased max_length\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_length=300)  # Adjust max_length as needed\n",
    "\n",
    "# Decode the output\n",
    "decoded_output = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Model Output:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8b06df6f3f43fb9838e9f51fceb43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:885: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38873699968e4972950d33dcb23d1591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/paligemma/configuration_paligemma.py:146: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.44, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9746f564f4704eefb6458380ce72d882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/45.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters pushed to Hugging Face Hub successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Load the processor\n",
    "model_id = \"google/paligemma-3b-pt-224\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Load the trained model checkpoint\n",
    "model_checkpoint_path = \"/teamspace/studios/this_studio/pali_gemma_derm/checkpoint-7690\"\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_checkpoint_path, device_map={\"\": 0})\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply the LoRA configuration to the trained model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Push the LoRA adapters to Hugging Face Hub\n",
    "peft_model.push_to_hub(\"brucewayne0459/Lora_pali_gemma\", use_auth_token=True)\n",
    "\n",
    "print(\"LoRA adapters pushed to Hugging Face Hub successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3644631140944b7a4cab3a36935575d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccac60a497af4722867a1160a09498bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/62.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266d98de55f7419a8aa6ba835cf82918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e046be40d3f471ab23b97bff00cb018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c73cc62a184734bd1593365d72a0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a5144fdd604353948d62493eb8efba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927f2e1bf857475485e51c2fcbf75f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7697cd4ca766490bb680c11c34763e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,PaliGemmaForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the base model\n",
    "base_model = PaliGemmaForConditionalGeneration.from_pretrained(\"google/paligemma-3b-pt-224\", device_map=\"auto\")\n",
    "\n",
    "# Load the PEFT model with LoRA adapters\n",
    "peft_model_id = \"/teamspace/studios/this_studio/pali_gemma_derm/checkpoint-7690\"\n",
    "peft_model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the LoRA adapters with the base model\n",
    "model_merge = peft_model.merge_and_unload()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged model locally\n",
    "merged_model_path = \"/teamspace/studios/this_studio/pali_gemma_merged\"\n",
    "#peft_model.save_pretrained(merged_model_path)\n",
    "\n",
    "model_merge.save_pretrained(merged_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:885: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c243a02ce34eb9871f1fe12db62f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108c4c14d56e491b812ba780d471e447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8a919bd1be49b38658a1f930765db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b7ae6379864a60aa537f255f5b8bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d98fb0f4834fa697c5350db83bab15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/brucewayne0459/paligemma_derm/commit/8c92e51a2f4a91c28c78a1797a7301bb1608b33a', commit_message='Upload PaliGemmaForConditionalGeneration', commit_description='', oid='8c92e51a2f4a91c28c78a1797a7301bb1608b33a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally, push the merged model to Hugging Face Hub\n",
    "model_merge.push_to_hub(\"brucewayne0459/paligemma_derm\", use_auth_token=True, safe_serialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c1ab5d9d5a49a6837feb577488d583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c85187ee7f43e299416ad4aeebe4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b62ee2b8574263857017181242b11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c20575b6dfa4c68816bee72956e32d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca575c62d859481bbfd3b6b34eac7148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9843c2611a8041c2be34828285278f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/teamspace/studios/this_studio/pali_gemma_merged/tokenizer_config.json',\n",
       " '/teamspace/studios/this_studio/pali_gemma_merged/special_tokens_map.json',\n",
       " '/teamspace/studios/this_studio/pali_gemma_merged/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoProcessor\n",
    "model_id = 'google/paligemma-3b-pt-224'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "merged_model_path = \"/teamspace/studios/this_studio/pali_gemma_merged\"\n",
    "\n",
    "# save the tokenizer\n",
    "processor.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "\n",
    "# push the tokenizer to hub\n",
    "#tokenizer.push_to_hub(new_hub_model_path, token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/hub.py:885: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d0a8d8be5b40928ae7509a1ece99c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/brucewayne0459/paligemma_derm/commit/62e892186457108d708cf30c46f985ed28bf3909', commit_message='Upload processor', commit_description='', oid='62e892186457108d708cf30c46f985ed28bf3909', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.push_to_hub(\"brucewayne0459/paligemma_derm\", use_auth_token=True, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668ca58d5f3a4a0ba58be9671c429b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: What is this skin condition?\n",
      "vitiligo\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model and processor\n",
    "model_id = \"brucewayne0459/paligemma_derm\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, device_map={\"\": 0})\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: Identify the skin condition?\n",
      "basal cell carcinoma\n"
     ]
    }
   ],
   "source": [
    "# Load a sample image and text input\n",
    "input_text = \"Identify the skin condition?\"\n",
    "input_image_path = \"/teamspace/studios/this_studio/ulcer-scc.jpg\"  # Replace with your actual image path\n",
    "input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "# Process the input\n",
    "inputs = processor(text=input_text, images=input_image, return_tensors=\"pt\", padding=\"longest\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set the maximum length for generation\n",
    "max_length = 512  # You can increase this as needed\n",
    "max_new_tokens = 50  # Controls how many new tokens are generated\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_length=max_length, max_new_tokens=max_new_tokens)\n",
    "\n",
    "# Decode the output\n",
    "decoded_output = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Model Output:\", decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
